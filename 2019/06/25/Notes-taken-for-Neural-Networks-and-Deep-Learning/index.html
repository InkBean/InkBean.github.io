<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/in.png">
	<link rel="shortcut icon" href="/img/in.png">
	
			    <title>
    InkBean's Blog
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="inkbean, Machine Learning, Deep Learning">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">AR</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Categories</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Research/">Research</a></li><li><a class="category-link" href="/categories/Study-Notes/">Study Notes</a></li><li><a class="category-link" href="/categories/日常/">日常</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">Archives</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2019/12/">December 2019</a></li><li><a class="archive-link" href="/archives/2019/11/">November 2019</a></li><li><a class="archive-link" href="/archives/2019/08/">August 2019</a></li><li><a class="archive-link" href="/archives/2019/07/">July 2019</a></li><li><a class="archive-link" href="/archives/2019/06/">June 2019</a>
	                    </li></ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="About">
		                About
		            </a>
		        </li>
		        
		        <li>
		            <a href="/friends/" title="Friends">
		                Friends
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="Gallery">
		                Gallery
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Tags">
		                Tags
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/InkBean" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(/gallery/qianqiao.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>Shallow Neural Networks</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="One-training-example"><a href="#One-training-example" class="headerlink" title="One training example"></a><strong>One training example</strong></h1><p>W[1] is a matrix made by stacking row vectors</p>
<p>Each row represents transpose of a column vector w(weights) used to compute each node. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z[1] = W[1] * x + b[1]</span><br></pre></td></tr></table></figure>

<p>x is a n x 1 matrix, as n stands for the number of features</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[1] = sigmoid(z[1])</span><br></pre></td></tr></table></figure>

<p>Move on to the next which is the output layer. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z[2] = W[2] * a[1] + b[2]</span><br></pre></td></tr></table></figure>

<p> z[2] is a 1 x 1 matrix, </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[2] = sigmoid(z[2])</span><br></pre></td></tr></table></figure>

<p>Apparently a[2] is a 1 x 1 matrix as well </p>
<p>x = a[0], always </p>
<p>yhat = a[2], in this case, since this is a two layer neural network</p>
<h1 id="Vectorizing-across-multiple-training-examples"><a href="#Vectorizing-across-multiple-training-examples" class="headerlink" title="Vectorizing across multiple training examples"></a><strong>Vectorizing across multiple training examples</strong></h1><blockquote>
<p>Z[1] = W[1] * X + b[1]</p>
<p>A[1] = sigmoid(Z[1])</p>
<p>Z[2] = W[2] * A[1] + b[2]</p>
<p>A[2] = sigmoid(z[2])</p>
</blockquote>
<p>X can also be written as A[0], you can see that each layer of a neural network is pretty much doing the same thing with only changes in the variables’ indices. </p>
<br>

<p>Z[1]: Horizontally, it scans through the training examples, vertically, it scans through the hidden units. </p>
<p>A[1] follows the same rule. </p>
<br>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z[1] = W[1].T * X + b[1]</span><br></pre></td></tr></table></figure>

<p>X is a n[0] x m matrix, as n stands for the number of features and m stands for the number of training examples. b is a n[1] by 1 matrix( a column vector ), but through broadcasting(a feature of python), it’s added to every one of the columns.</p>
<p>Notice that after vectorization, the ‘a’, ‘z’, and ‘x’ become capital letters ‘A’, ‘Z’, and ‘X’ .</p>
<h1 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a><strong>Activation functions</strong></h1><p><strong>Sigmoid function</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = Sigmoid(z) = 1 / (1 + e^-z)</span><br></pre></td></tr></table></figure>

<br>

<p>An activation function used to calculate A can be a non-linear function other than sigmoid function. </p>
<p><strong>e.g.</strong> Hyperbolic tangent function, ReLU, leaky ReLU…</p>
<br>

<p><strong>Hyperbolic Tangent Function</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = tanh(z) = (e^z - e^-z) / (e^z + e^-z)</span><br></pre></td></tr></table></figure>

<p>Mathematically, it’s just a shifted version of the sigmoid function. It ranges from 1 to -1.</p>
<p>It almost always does a better job, since it has mean closer to 0 rather than 0.5 like that of the sigmoid function. ( Further explanation will be talked about in the second course in this series. This is the first course btw. </p>
<p>One exception: Binary classification(output is 0 or 1). May still use sigmoid function for the output layer, even though the activation function(s) used in the hidden layer might be different. Noted that you may have distinct activation function for different layers. </p>
<br>

<p><strong>Rectified Linear Unit | ReLU</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = max(0, z)</span><br></pre></td></tr></table></figure>

<p>The derivative is 1 when z is positive and 0 when it’s negative. </p>
<p>Usually, <strong>ReLU works much better</strong> than either tanh or sigmoid. When z is too big or too small, the derivative of the tanh or sigmoid function can be really small(close to zero) which slows down learning, whereas ReLU can avoid this issue(enough of the units will have values of z greater than zero). </p>
<h1 id="Why-do-you-need-non-linear-activation-functions"><a href="#Why-do-you-need-non-linear-activation-functions" class="headerlink" title="Why do you need non-linear activation functions?"></a><strong>Why do you need non-linear activation functions?</strong></h1><p>Without using a non-linear activation function(or not using an activation function at all), no matter how many layers of neural network you have, what you’re doing is still just computing a linear function of X. This is because a composition of linear functions is still itself a linear function. </p>
<p><strong>e.g.</strong> If you have a hidden layer computed using linear functions and have a sigmoid function applied in the output layer, then it’s, as Andrew puts, “more more expressive than” a standard logistic regression with no hidden layer. </p>
<br>

<p>One of the extremely rare cases you may use a linear activation function: if you’re doing a regression problem of which the output is a real number, you might apply this linear function in the output layer. </p>
<p><strong>e.g.</strong> predict housing price(a classic example mentioned in the first week of Andrew’s Machine Learning class lol)</p>
<h1 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a><strong>Derivatives of activation functions</strong></h1><p><strong>Sigmoid:</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g(z) = 1 / (1 + e^-z)</span><br><span class="line"></span><br><span class="line">g&apos;(z) = dg(z)/dz </span><br><span class="line"></span><br><span class="line">		= e^-z * (1 / (1 + e^-z)^2 )  // for those who want to know how it&apos;s derived. </span><br><span class="line"></span><br><span class="line">		= 1 / (1 + e^-z) * (1 - (1 / (1 + e^-z) ) )</span><br><span class="line"></span><br><span class="line">		= g(z) * (1 - g(z))</span><br><span class="line"></span><br><span class="line">		= a * (1 - a)</span><br></pre></td></tr></table></figure>

<br>

<p><strong>Tanh:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g(z) = (e^z - e^-z) / (e^z + e^-z)</span><br><span class="line"></span><br><span class="line">g&apos;(z) = 1 - ( tanh(z) )^2  // its derivation is a bit more complicated</span><br><span class="line"></span><br><span class="line">		= 1 - g(z)^2</span><br><span class="line"></span><br><span class="line">		= 1 - a^2</span><br></pre></td></tr></table></figure>

<br>

<p><strong>ReLU:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(z) = max(0, z)</span><br><span class="line"></span><br><span class="line">g&apos;(z) = 0 if z &lt; 0, 1 if z &gt; 0</span><br></pre></td></tr></table></figure>

<p>When z exactly equals to 0, which is very unlikely, you can set it to either 0 or 1 and it doesn’t really matter.</p>
<br>

<p><strong>Leaky ReLU:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(z) = max(0.01z, z)</span><br><span class="line"></span><br><span class="line">g&apos;(z) = 0.01 if z &lt; 0, 1 if z &gt; 0</span><br></pre></td></tr></table></figure>

<h1 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a><strong>Gradient descent for neural networks</strong></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Parameters:			W[1]		b[1]			W[2]			b[2]</span><br><span class="line"></span><br><span class="line">Dimensions: 	(n[1] x n[0]) 		(n[1] x 1)		(n[2] x n[1])		(n[2] x 1)</span><br></pre></td></tr></table></figure>

<p>n stands for the number of nodes in the [i]th layer. </p>
<p>nx = n[0],</p>
<p>and n[2] = 1(since it’s the output layer and there’s only one output)</p>
<br>

<p><strong>Cost function:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J( W[1], b[1], W[2], b[2]) = (1 / m) * sum of the Loss function of (yhat, y) over the m training examples</span><br></pre></td></tr></table></figure>

<br>

<p><strong>Gradient descent:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W[1] = 1 - alpha * dW[1] // don&apos;t forget the learning rate alpha!</span><br><span class="line"></span><br><span class="line">b[1] = 1 - alpha * db[1]</span><br></pre></td></tr></table></figure>

<br>

<p><strong>Back propagation</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">dZ[2] = A[2] - Y</span><br><span class="line"></span><br><span class="line">dW[2] = (1 / m) * np.dot( dZ[2],  A[1].T )                   </span><br><span class="line">// This &apos;np.dot&apos; is matrix wise multiplication</span><br><span class="line"></span><br><span class="line">db[2] = (1 / m) * np.sum(dZ[2], axis = 1, keepdims = true)     </span><br><span class="line">// This &apos;np.sum&apos; is summing up horizontally</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dZ[1] = np.dot( W[2].T, dZ[2] ) * g[1]&apos;(Z[1])         // n[1] by m matrix                  </span><br><span class="line">// This &apos;*&apos; represents element wise multiplication</span><br><span class="line"></span><br><span class="line">                         </span><br><span class="line"></span><br><span class="line">dW[1] = (1 / m) * np.dot(dZ[1], X.T )</span><br><span class="line"></span><br><span class="line">db[1] = (1 / m) * np.sum(dZ[1], axis = 1, keepdims = true)</span><br></pre></td></tr></table></figure>

<p><strong>P.S.</strong> After this part, there’s an optional video on the math of the derivation of the vectorization above, but it doesn’t explain everything/</p>
<p>I may write more about the math later. </p>
<h1 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a><strong>Random initialization</strong></h1><p><strong>Symmetry breaking problem:</strong></p>
<p>If you initialize all values of W(weights) to 0, then all the hidden units will start off computing the same function, getting the same value, and therefore receive the same update of W(i.e. same dW)</p>
<br>

<p><strong>Solution:</strong> ( Assuming that, for instance, there’re 3 nodes in the hidden layer and 2 features for X</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W[1] = np.ramdom.randn( (3, 2) ) * 0.01  </span><br><span class="line">b[1] = np.zero( (3, 1) )  // notice that you can still initialize biases to 0s though</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W[2] = np.ramdom.randn( (1, 3) ) * 0.01  </span><br><span class="line">b[2] = np.zero( (1, 1) )</span><br></pre></td></tr></table></figure>

<p>We tend to initialize it to <strong>very small random values.</strong> </p>
<p>If we initialize the weights to big values, the values of z computed will be very large because of that. A large value of z will end up locating, if the activation function is a sigmoid or tanh function, at a place where it has a very small value of derivative/slope/gradient(close to 0), which will slow down gradient descent a lot. </p>
<p>When we’re training a very deep neural network, we should probably choose a constant other than 0.01. This part will be elaborated in videos in week 4.  </p>
<h1 id="Programming-assignment"><a href="#Programming-assignment" class="headerlink" title="Programming assignment"></a><strong>Programming assignment</strong></h1><p>In the last part of the programming assignment, if you play around with the number of the hidden units, you may find that it’s not the case that the more hidden units you have the more accurate your NN model is. This leads to the discussion of the problem of <strong>overfitting.</strong></p>
<blockquote>
<p>If the number of hidden units is too big, it may cause overfitting, while if it’s too small, the model may fail to capture the details(underfitting). </p>
</blockquote>
<blockquote>
<p>This tutorial <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a> has shown how having too many layers may cause overfitting. </p>
</blockquote>
<blockquote>
<p>Training your NN over the same training set over and over again for way too many times may also cause overfitting even if your NN doesn’t fall into the traps above. </p>
</blockquote>
<p>One good quality of an NN is the capability to <strong>generalize</strong>. Overfitting is the opposite, however. </p>
<p><strong>Regularization</strong> can provide a solution to the problem of overfitting without decreasing the number of hidden units. It has been taught in the 3rd week of Andrew’s Machine Learning course.</p>
<h1 id="Some-random-things-I-want-to-mention"><a href="#Some-random-things-I-want-to-mention" class="headerlink" title="Some random things I want to mention"></a><strong>Some random things I want to mention</strong></h1><p>I wrote a brief summary for week 2 of this course as well, but decided not to publish it.</p>
<p>Instead, I may write an article some time later, elaborating on the whole process of training an NN and the mathematics behind it(from doing the calculus to understanding cross-entropy loss). </p>
<p>As I myself am not that familiar with it, presumably writing it out may also boost my own understanding on this topic. </p>
<p>我发现自己在学习的很多时候都处于一种似懂非懂的状态，这和学习心态必然有关系，方法上也出了问题。高中上理科课的时候从来不记笔记，课下作业也就糊弄过去。能看懂题会套公式就能拿个不错的分。高三时候自学的几门理科AP也都是考前速成，遵循会做题就行的准则。</p>
<p>刚上大学的时候也没怎么变，会做题，考个高分就满足了。虽然心里知道自己对概念还是没有完全掌握</p>
<p>然而最近愈加意识到公式和概念背后的意义和想法的重要性，也感到了需要改善自己这一直以来半吊子的心态和对知识的理解的急迫性。</p>
<p>说到底还是要提升自己姿势水平。似懂非懂无非是没怎么懂，自欺欺人是对自己的不负责任。</p>
<p>把学的东西在理解过后整理并写出来算是一种检验自己理解的方式。</p>
<p>解释不出来就是实际上没懂，而当确实解释出来了的时候便算是确实懂了吧。</p>
<hr>
<p>Week 3 in Andrew Ng’s <a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome" target="_blank" rel="noopener">‘Neural Networks and Deep Learning’</a> on Coursera.</p>
<p>Cover by <a href="https://www.pixiv.net/member.php?id=1236873" target="_blank" rel="noopener">カオミン</a> on Pixiv</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">Comments</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'inkbin'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2019/06/25/Notes-taken-for-Neural-Networks-and-Deep-Learning/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2019/06/25/Notes-taken-for-Neural-Networks-and-Deep-Learning/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//inkbin.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy; <a href="https://inkbean.github.io" style="border-bottom: none;">InkBean's Blog</a></li>
                <li>Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Theme: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_uv">有<span id="busuanzi_value_site_uv"></span>个小伙伴来过</span>
			
        </div>
    </div>
</body>



 	
</html>
