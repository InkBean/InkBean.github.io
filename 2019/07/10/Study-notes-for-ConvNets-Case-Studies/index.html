<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/in.png">
	<link rel="shortcut icon" href="/img/in.png">
	
			    <title>
    InkBean's Blog
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="inkbean, Machine Learning, Deep Learning">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">AR</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Categories</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Research/">Research</a></li><li><a class="category-link" href="/categories/Study-Notes/">Study Notes</a></li><li><a class="category-link" href="/categories/日常/">日常</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">Archives</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2019/12/">December 2019</a></li><li><a class="archive-link" href="/archives/2019/11/">November 2019</a></li><li><a class="archive-link" href="/archives/2019/08/">August 2019</a></li><li><a class="archive-link" href="/archives/2019/07/">July 2019</a></li><li><a class="archive-link" href="/archives/2019/06/">June 2019</a>
	                    </li></ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="About">
		                About
		            </a>
		        </li>
		        
		        <li>
		            <a href="/friends/" title="Friends">
		                Friends
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="Gallery">
		                Gallery
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Tags">
		                Tags
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/InkBean" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(/gallery/yr.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>ConvNets Case Studies</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="Case-studies"><a href="#Case-studies" class="headerlink" title="Case studies"></a><strong>Case studies</strong></h1><hr>
<h3 id="Classical-networks"><a href="#Classical-networks" class="headerlink" title="Classical networks"></a><strong>Classical networks</strong></h3><p><strong>Lenet - 5:</strong></p>
<p>Recognize hand-written digits(gray scale).</p>
<p>This example is pretty similar to the architecture mentioned last week.</p>
<p>Used Sigmoid and Tanh</p>
<p><strong>Alex Net:</strong></p>
<p>Similar to Lenet but much bigger</p>
<p>Used ReLU</p>
<p><strong>VGG16:</strong></p>
<p>Conv = 3 x 3 filter, s = 1, same</p>
<p>Max-Pool = 2 x 2 filter, s = 2</p>
<br>

<h3 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a><strong>ResNets</strong></h3><p>Residual Networks</p>
<p>Main path:</p>
<blockquote>
<p>a[l] → Linear → ReLU → a[l + 1]  → Linear → ReLU →a[l + 2]</p>
</blockquote>
<p>previously, we calculate a[l + 2] by</p>
<blockquote>
<p>a[l + 2] = g[l + 2](z[l + 2])</p>
</blockquote>
<p>but now we have a “shortcut”/“skip” connection</p>
<blockquote>
<p>a[l + 2] = g[l + 2](z[l + 2] + <strong>a[l]</strong>)</p>
<p><strong>a[l]</strong> is injected after the linear block, before the non-linearity</p>
<p>for every two layers, we have this change and form a residual block</p>
</blockquote>
<br>

<p>Motivation:</p>
<p>In practice, having more layers doesn’t necessarily decrease the training error. As #layers increases, the training error will go down and then turn to go up at some point. Having a ResNet prevents this from happening even if a very deep neural network is trained.</p>
<br>

<h3 id="Why-ResNets-work"><a href="#Why-ResNets-work" class="headerlink" title="Why ResNets work"></a><strong>Why ResNets work</strong></h3><blockquote>
<p>a[l + 2] = g[l + 2](W[l + 1] * a[l + 1] + b[l + 1] + a[l]) = g[l + 2](z[l +2] + a[l]) </p>
<p>W and b can be really small as we apply weight decay(L2 regularization)</p>
<p>Assuming W = b = 0 in this case</p>
<p>a[l + 2] = a[l]</p>
<p>It makes learning the identity function easier and doesn’t hurt performance</p>
</blockquote>
<p>Notice that we add z[l + 2] and a[l], which assumes that they have the same dimension(that’s why we may apply lots of same convolution layers). If they do not, we may multiply a[l] by a matrix W which makes it have the same dimension as z[l + 2]</p>
<br>

<h3 id="Networks-in-Networks-and-1-x-1-Convolution"><a href="#Networks-in-Networks-and-1-x-1-Convolution" class="headerlink" title="Networks in Networks and 1 x 1 Convolution"></a><strong>Networks in Networks and 1 x 1 Convolution</strong></h3><p>e.g.</p>
<blockquote>
<p>Input a 28 x 28 x 192</p>
<p>1 x 1 x 192, 32 filters</p>
<p>then apply ReLU</p>
<p>end up 28 x 28 x 32(nH, nW, nC)</p>
<p>It adds non-lineartiy to the network, and allows you to change #channels without changing height and width.</p>
</blockquote>
<br>

<h3 id="Inception-Network-Motivation"><a href="#Inception-Network-Motivation" class="headerlink" title="Inception Network Motivation"></a><strong>Inception Network Motivation</strong></h3><p>Motivation:</p>
<blockquote>
<p>Want to do the convolutions and pooling, but we run into <strong>the problem of computational cost</strong></p>
</blockquote>
<blockquote>
<p><strong>e.g.</strong> 28 x 28 x 192 Convolve 5 x 5 x 192(32 filters) ≈ 120 Million</p>
</blockquote>
<p>But by using a 1 x 1 convolution in between,</p>
<blockquote>
<p>28 x 28 x 192 Convolve 1 x 1 x 192(16 filters) ≈ 2.4 Million</p>
<p>Gives 28 x 28 x 16, a bottleneck layer(shrink the representation before increasing the size)</p>
<p>28 x 28 x 16 * 5 x 5 x 16(32 filters) ≈ 10.0 Million</p>
<p>Reduce from 120Million to 12.4Million approximately.</p>
</blockquote>
<br>

<h3 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a><strong>Inception Network</strong></h3><p>Inception Module</p>
<blockquote>
<p>What it does is to put all of the modules(convolution, pooling) together and concatenate them all.</p>
</blockquote>
<p>You may also have some side branches(using softmax) to predict the results, to show that the predictions by the hidden layers aren’t too bad. This also has a regularizing effect</p>
<p>Fun fact: </p>
<blockquote>
<p>Got inspiration from <em>Inception</em></p>
<p>Cited the meme “We need to go deeper”</p>
</blockquote>
<h1 id="Practical-advices-for-using-ConvNets"><a href="#Practical-advices-for-using-ConvNets" class="headerlink" title="Practical advices for using ConvNets"></a><strong>Practical advices for using ConvNets</strong></h1><hr>
<h3 id="Using-Open-Source-Implementation"><a href="#Using-Open-Source-Implementation" class="headerlink" title="Using Open-Source Implementation"></a><strong>Using Open-Source Implementation</strong></h3><p>Github lol.</p>
<br>

<h3 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a><strong>Transfer Learning</strong></h3><p>You can freeze some(shallower) layers and train the later layers.</p>
<p>If you have more data, the number of layers you freeze might be smaller, the number of layers you train on top can be bigger. Or you may just use the whole thing (the network you cloned) as initialization.</p>
<br>

<h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a><strong>Data augmentation</strong></h3><p>For CV problems, we just want as many data as possible.</p>
<p>Data augmentation can help.</p>
<p><strong>Common augmentation method:</strong></p>
<blockquote>
<p>Mirroring</p>
<p>Random cropping(isn’t perfect but works well in practice)</p>
<p>Rotation, sheering etc. may also be used</p>
<p>Color shifting is also a commonly used technique</p>
<p>Take different values of R,G,B(<strong>e.g.</strong> +20, -20, +20) </p>
<p>The identity of label y should stay the same</p>
<p>Make your learning algorithm more rubust to color changes.</p>
<p>Advanced:</p>
<ul>
<li>PCA, adds a lot to R and B but little to G(taught in Andrew’s ML class)</li>
</ul>
</blockquote>
<p>A CPU thread to load data and do image distortion. This and training can run in parrallel.</p>
<br>

<h3 id="State-of-Computer-Vision"><a href="#State-of-Computer-Vision" class="headerlink" title="State of Computer Vision"></a><strong>State of Computer Vision</strong></h3><p>Data vs. hand-engineering</p>
<p>More hand-engineering when we don’t have a lot of data.</p>
<p><strong>Two sources of knowledge</strong></p>
<blockquote>
<ol>
<li>Labeled data</li>
<li>hand engineered features/network architecture/other components</li>
</ol>
</blockquote>
<p>Tips for doing well on benchmark/ winning competitions.(don’t often use them in production though)</p>
<blockquote>
<p>Ensembling</p>
<ul>
<li>Train several networks independently and average their outputs(yhat but not weights)</li>
</ul>
<p>Multi-crop at test time</p>
<ul>
<li>Run classifier on multiple versions of test image and average results.</li>
</ul>
</blockquote>
<p>Use Open Source Code</p>
<blockquote>
<p>Use architectures of networks published in literature</p>
<p>Use open source implementations if possible</p>
<p>Use pre-trained models and fine-tune on your dataset</p>
</blockquote>
<hr>
<p>Week 2 in Andrew Ng’s <a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome" target="_blank" rel="noopener">Convolutional Neural Networks</a> on Coursera.</p>
<p>Cover by <a href="https://www.pixiv.net/member_illust.php?id=59205" target="_blank" rel="noopener">悠里</a> on Pixiv.</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">Comments</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'inkbin'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2019/07/10/Study-notes-for-ConvNets-Case-Studies/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2019/07/10/Study-notes-for-ConvNets-Case-Studies/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//inkbin.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy; <a href="https://inkbean.github.io" style="border-bottom: none;">InkBean's Blog</a></li>
                <li>Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Theme: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_uv">有<span id="busuanzi_value_site_uv"></span>个小伙伴来过</span>
			
        </div>
    </div>
</body>



 	
</html>
