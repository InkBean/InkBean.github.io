<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/in.png">
	<link rel="shortcut icon" href="/img/in.png">
	
			    <title>
    InkBean's Blog
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="inkbean, Machine Learning, Neural Network, Deep Learning">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">InkBean</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Learning/">Learning</a></li><li><a class="category-link" href="/categories/日常/">日常</a></li><li><a class="category-link" href="/categories/玩耍/">玩耍</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">归档</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2019/08/">August 2019</a></li><li><a class="archive-link" href="/archives/2019/07/">July 2019</a></li><li><a class="archive-link" href="/archives/2019/06/">June 2019</a>
	                    </li></ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="关于">
		                关于
		            </a>
		        </li>
		        
		        <li>
		            <a href="/friends/" title="友達">
		                友達
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/InkBean" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(/gallery/sk.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>Study Notes for &#39;Recurrent Neural Networks&#39;</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p>Week 1 in Andrew Ng’s <a href="https://www.coursera.org/learn/nlp-sequence-models/home/welcome" target="_blank" rel="noopener">Sequence Models</a> on Coursera.</p>
<br>

<h1 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a><strong>Recurrent Neural Networks</strong></h1><hr>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a><strong>Notation</strong></h3><p>x(i)&lt;t&gt; = the t_th feature of the i_th training example</p>
<p>Tx(i) = the length of the sequence of the training example</p>
<p>y(i)&lt;t&gt; = the t_th feature of the i_th output sequence</p>
<p>Ty(i) = the length of the sequence of the output</p>
<br>

<p><strong>Vocabulary/Dictionary:</strong></p>
<blockquote>
<p>A vector that stores the words in indexes </p>
</blockquote>
<p>Use one hot representations to represent the words. </p>
<br>

<h3 id="Recurrent-Neural-Network-Model"><a href="#Recurrent-Neural-Network-Model" class="headerlink" title="Recurrent Neural Network Model"></a><strong>Recurrent Neural Network Model</strong></h3><p>Why not a standard neural network?</p>
<p>Problems:</p>
<blockquote>
<p>Inputs, outputs can be different lengths in different examples</p>
<p>Doesn’t share features learnt across different positions of text</p>
</blockquote>
<br>

<p>RNN:</p>
<blockquote>
<p>Each time, feed in x&lt;t&gt; then output y&lt;t&gt;.</p>
<p>At each time step, it passes the activations to the next time step.</p>
<p>Same parameter Wax, Waa, Way for each step.</p>
</blockquote>
<p>Though it makes use of earlier words, the weakness of this unidirectional architecture is that doesn’t make use of later words in the sequence.</p>
<br>

<p>To forward propagate,</p>
<blockquote>
<p>a&lt;0&gt; = 0</p>
<p>a&lt;1&gt; = g( Waa * a&lt;0&gt; + Wax * x&lt;1&gt; + ba )    // tanh is a pretty common choice, ReLU is sometimes used</p>
<p>yhat&lt;1&gt; = g( Wya * a&lt;1&gt; + by)        // the activation function here depends on the output task</p>
</blockquote>
<p>More generally,</p>
<blockquote>
<ol>
<li><p>a&lt;t&gt; = g( Waa * a&lt;t - 1&gt; + Wax * x&lt;t&gt; + ba )</p>
</li>
<li><p>yhat&lt;t&gt; = g( Wya * a&lt;t&gt; + by )</p>
</li>
</ol>
<p>We can simplify the notation of the 1st step to</p>
<p>a&lt;t&gt; = g( Wa * [a&lt;t - 1&gt;, x] + ba)</p>
<p>​    <strong>Wa</strong> is made by stacking <strong>Waa</strong> and <strong>Wax</strong> side by side horizontally</p>
<p>​    <strong>[a&lt;t - 1&gt;, x]</strong> = <strong>a&lt;t - 1&gt;</strong> and <strong>x</strong> stacked together vertically</p>
<p>The 2nd step to</p>
<p>yhat&lt;t&gt; = g( Wy * a&lt;t&gt; + by )</p>
</blockquote>
<p>To summarize, </p>
<blockquote>
<p><strong>a&lt;t&gt; = g( Wa * [a&lt;t - 1&gt;, x] + ba)</strong></p>
<p><strong>yhat&lt;t&gt; = g( Wy * a&lt;t&gt; + by )</strong></p>
</blockquote>
<br>

<h3 id="Back-propagation-through-time"><a href="#Back-propagation-through-time" class="headerlink" title="Back propagation through time"></a><strong>Back propagation through time</strong></h3><blockquote>
<p>L&lt;t&gt; (yhat&lt;t&gt;, y&lt;t&gt;) = y&lt;t&gt; * log(yhat&lt;t&gt;) - (1 - y&lt;t&gt;) * log( 1 - yhat&lt;t&gt;)</p>
<p>L(yhat, y) = sum over the L&lt;t&gt;(yhat&lt;t&gt;, y&lt;t&gt;)</p>
</blockquote>
<br>

<h3 id="Different-types-of-RNNs"><a href="#Different-types-of-RNNs" class="headerlink" title="Different types of RNNs"></a><strong>Different types of RNNs</strong></h3><p><strong>Many-to-many architecture</strong></p>
<p>When Tx = Ty</p>
<p><strong>E.g.</strong> Name entity recognition</p>
<p>When Tx ≠ Ty</p>
<p><strong>E.g.</strong> Machine translation: An encoder that takes an input and a decoder that generates output translation in a different language.</p>
<br>

<p><strong>Many-to-one architecture</strong></p>
<p><strong>E.g.</strong> Sentiment classification: x = text, y = number</p>
<br>

<p><strong>One-to-one architecture</strong></p>
<p>Basically just a standard generic NN</p>
<br>

<p><strong>One-to-many architecture</strong></p>
<p><strong>E.g.</strong> Music generation</p>
<br>

<h3 id="Language-model-and-sequence-generation"><a href="#Language-model-and-sequence-generation" class="headerlink" title="Language model and sequence generation"></a><strong>Language model and sequence generation</strong></h3><p>Language modeling</p>
<blockquote>
<p>Tells P(sentence), the probability of a sentence appearing</p>
</blockquote>
<p>Output sentences that’re likely</p>
<p><strong>Training set:</strong> a large corpus(body/set) of English text</p>
<p>Process</p>
<blockquote>
<p>Take a sentence, tokenize each word(convert each word to a one-hot vector)</p>
<p>Sometimes a “EOS”(End of sentence) is also appended to denote where the sentence ends. </p>
</blockquote>
<p>What if a word isn’t included in your vocabulary appears in a sentence?</p>
<blockquote>
<p>Take the word and replace it with a unique token UNK(Unknown words)</p>
</blockquote>
<br>

<p>Each step, it outputs what’s the chance of a word appearing, given the previous words. </p>
<p><strong>Loss function:</strong></p>
<blockquote>
<p>L&lt;t&gt;(yhat&lt;t&gt;, y&lt;t&gt;) = sum over all the categories (y&lt;t&gt; * log(yhat&lt;t&gt;))</p>
<p>L = sum over all L&lt;t&gt;(yhat&lt;t&gt;, y&lt;t&gt;)</p>
</blockquote>
<p>The probability of a three word sentence</p>
<p>P(y&lt;1&gt;, y&lt;2&gt;, y&lt;3&gt;) = P(y&lt;1&gt;) * P(y&lt;2&gt; | y&lt;1&gt;) * P(y&lt;3&gt; | y&lt;1&gt;, y&lt;2&gt;) </p>
<br>

<h3 id="Sampling-novel-sequences"><a href="#Sampling-novel-sequences" class="headerlink" title="Sampling novel sequences"></a><strong>Sampling novel sequences</strong></h3><p>Sampling:</p>
<p>Generate a randomly chosen sentence, np.sample.choice</p>
<br>

<p>You can also create a character-level language model</p>
<blockquote>
<p>Advantage: won’t worry about UNK</p>
<p>Disadvantage: much longer sequences, more computational expensive to train</p>
</blockquote>
<br>

<p>Now you can build an RNN to look at a corpus of English text, and build a language model, then sample from the language model you’ve trained.</p>
<p>Sequence generation: </p>
<p><strong>E.g.</strong> Shapespeare poems</p>
<br>

<h3 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a><strong>Vanishing gradients with RNNs</strong></h3><p>A basic RNN finds it hard to capture long-term dependency</p>
<br>

<p>Exploding gradient: </p>
<p>Just apply gradient clipping(re-scale)</p>
<br>

<h3 id="Gated-Recurrent-Units-GRU"><a href="#Gated-Recurrent-Units-GRU" class="headerlink" title="Gated Recurrent Units(GRU)"></a><strong>Gated Recurrent Units(GRU)</strong></h3><p>c = memory cell</p>
<p>The <strong>cat</strong>, which already ate……., <strong>was</strong> full.</p>
<p>For instance, it remembers that the cat was singular</p>
<p>c&lt;t&gt; = a&lt;t&gt;</p>
<br>

<p><strong>c~&lt;t&gt;</strong> is a candidate for replacing c&lt;t&gt;</p>
<blockquote>
<p>c~&lt;t&gt; = tanh( Wc * [c&lt;t - 1&gt;, x&lt;t&gt;] + bc )</p>
</blockquote>
<p><strong>Gate:</strong></p>
<p>Γu, used to decide when to update c&lt;t&gt;</p>
<blockquote>
<p>Γu = σ(Wu * [a&lt;t - 1&gt;, x&lt;t&gt;] + bu) // sigmoid function</p>
<p>Γu pretty much equals to 0 or 1 for most of the time.</p>
</blockquote>
<p>When Γu is 0, set c&lt;t&gt; equal to the old value(c&lt;t - 1&gt;). </p>
<blockquote>
<p>c&lt;t&gt; = Γu <strong><strong>* c~&lt;t&gt; + (1 - Γu) *</strong></strong> c&lt;t - 1&gt;</p>
<p>The <strong>‘*’</strong> here is element-wise. </p>
</blockquote>
<br>

<p><strong>To summarize,</strong></p>
<blockquote>
<p>c~&lt;t&gt; = tanh( Wc * [ Γr * c&lt;t - 1&gt;, x&lt;t&gt;] + bc )</p>
<p>Γu = σ(Wu * [c&lt;t - 1&gt;, x&lt;t&gt;] + bu)</p>
<p>Γr = σ(Wr * [c&lt;t - 1&gt;, x&lt;t&gt;] + br) // Γr is to determine the relevance of c&lt;t - 1&gt;</p>
<p>c&lt;t&gt; = Γu <strong><strong>* c~&lt;t&gt; + (1 - Γu) *</strong></strong> c&lt;t - 1&gt;</p>
<p>a&lt;t&gt; = c&lt;t&gt;</p>
</blockquote>
<p>For the signal to backpropagate without vanishing, we need c&lt;t&gt; to be highly dependant on c&lt;t−1&gt;.</p>
<br>

<h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory(LSTM)"></a><strong>Long Short Term Memory(LSTM)</strong></h3><blockquote>
<p>c~&lt;t&gt; = tanh( Wc * [ a&lt;t - 1&gt;, x&lt;t&gt;] + bc )</p>
<p>Γu = σ(Wu * [a&lt;t - 1&gt;, x&lt;t&gt;] + bu)            Update Gate</p>
<p>Γf = σ(Wf * [a&lt;t - 1&gt;, x&lt;t&gt;] + bf)                 Forget Gate</p>
<p>Γo = σ(Wo * [a&lt;t - 1&gt;, x&lt;t&gt;] + bo)                Output Gate</p>
<p>c&lt;t&gt; = Γu * c~&lt;t&gt; + Γf * c&lt;t - 1&gt; //    Γf gives an option of keeping c&lt;t - 1&gt;</p>
<p>a&lt;t&gt; = Γo * tanh( c&lt;t&gt; )</p>
</blockquote>
<br>

<p>Peephole connection:</p>
<blockquote>
<p>Sometimes c&lt;t - 1&gt; will also be put into the matrix to calculate the gates.</p>
</blockquote>
<br>

<h3 id="Bidirectional-RNN-BRNN"><a href="#Bidirectional-RNN-BRNN" class="headerlink" title="Bidirectional RNN(BRNN)"></a><strong>Bidirectional RNN</strong>(BRNN)</h3><p>Getting information from the future</p>
<p>Acyclic graph</p>
<blockquote>
<p>yhat&lt;t&gt; = g( Wy * [a→&lt;t&gt;, a←&lt;t&gt;] + by )</p>
<p>This allows the output to take both <strong>information from the past ** and that from the present</strong>.</p>
</blockquote>
<p>Disadvantage:</p>
<blockquote>
<p>Need to take account of the entire speech utterance. </p>
</blockquote>
<br>

<h3 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a><strong>Deep RNNs</strong></h3><p>a[l]&lt;t&gt; denotes activation in the l_th layer of the  t_th word. </p>
<blockquote>
<p>a[l]&lt;t&gt; = g( Wa * [ a[l]&lt;t - 1&gt;, a[l - 1]&lt;t&gt; ] + ba[l])</p>
</blockquote>
<p>For RNNs, </p>
<blockquote>
<p>Deep recurrent layers isn’t very common, 3 layers is already quite deep. </p>
<p>It’s because they have temporal dimension which already makes the network really big. </p>
</blockquote>
<hr>
<p>Cover by <a href="https://www.pixiv.net/member_illust.php?id=189732" target="_blank" rel="noopener">爽々</a> on Pixiv</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">View Comments</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'inkbin'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2019/07/20/Study-Notes-for-Recurrent-Neural-Networks/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2019/07/20/Study-Notes-for-Recurrent-Neural-Networks/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//inkbin.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy; <a href="https://inkbean.github.io" style="border-bottom: none;">InkBean's Blog</a></li>
                <li>Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Theme: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_uv">有<span id="busuanzi_value_site_uv"></span>个小伙伴来过</span>
			
        </div>
    </div>
</body>



 	
</html>
