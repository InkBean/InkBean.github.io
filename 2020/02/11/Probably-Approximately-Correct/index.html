<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/in.png">
	<link rel="shortcut icon" href="/img/in.png">
	
			    <title>
    AR's Blog
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="inkbean, Machine Learning, Deep Learning">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">AR</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Categories</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Research/">Research</a></li><li><a class="category-link" href="/categories/Study-Notes/">Study Notes</a></li><li><a class="category-link" href="/categories/日常/">日常</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">Archives</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2020/02/">February 2020</a></li><li><a class="archive-link" href="/archives/2019/12/">December 2019</a></li><li><a class="archive-link" href="/archives/2019/11/">November 2019</a></li><li><a class="archive-link" href="/archives/2019/08/">August 2019</a></li><li><a class="archive-link" href="/archives/2019/07/">July 2019</a></li><li><a class="archive-link" href="/archives/2019/06/">June 2019</a>
	                    </li></ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="About">
		                About
		            </a>
		        </li>
		        
		        <li>
		            <a href="/friends/" title="Friends">
		                Friends
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="Gallery">
		                Gallery
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Tags">
		                Tags
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/InkBean" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(/gallery/kq.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>PAC in a nutshell</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="probably-approximately-correct"><strong>Probably Approximately Correct</strong></h1>
<hr>
<p>Notations:</p>
<p><span class="math inline">\(H\)</span> : Class of hypothesis</p>
<p><span class="math inline">\(D\)</span> : Distribution</p>
<p><span class="math inline">\(f\)</span> : Labeling function of <span class="math inline">\(D\)</span></p>
<p><span class="math inline">\(S\)</span> : Sample</p>
<p><span class="math inline">\(m\)</span> : Sample size</p>
<p><span class="math inline">\(L\)</span> : Loss function</p>
<p><br></p>
<hr>
<p><strong>Empirical Risk Minimization(ERM)</strong>:</p>
<p>A principle that is to find a predictor <span class="math inline">\(h\)</span> that minimizes empirical risk <span class="math inline">\(L_S(h)\)</span>.</p>
<p><br></p>
<p>When <strong>realizability</strong> assumption holds, that is</p>
<ul>
<li><span class="math inline">\(\exists h_* \in H\)</span> such that <span class="math inline">\(L_{D,f}(h_*) = 0\)</span></li>
<li>This means that it's possible to have a hypothesis that correctly predicts all the labels in the distribution <span class="math inline">\(D\)</span></li>
</ul>
<p><em>Corollary</em>: <span class="math inline">\(P(L_S(h*) = 0) = 1\)</span>, with any <span class="math inline">\(S\)</span> sampled from <span class="math inline">\(D\)</span></p>
<p>the predictor <span class="math inline">\(h\)</span> returned by the ERM always has an empirical risk <span class="math inline">\(L_S(h)\)</span> = 0.</p>
<p><br></p>
<p>And the examples in the sample are sampled following</p>
<p><strong>i.i.d. assumption</strong>:</p>
<ul>
<li>i.i.d. stands for &quot;independently and identically distributed&quot;</li>
<li><strong>i.e.</strong> all the examples in the sample follow this assumption with respect to the distribution <span class="math inline">\(D\)</span></li>
<li>we denote it as <span class="math inline">\(S\)</span> ~ <span class="math inline">\(D^m\)</span></li>
</ul>
<p><br></p>
<p>As the i.i.d. assumption holds, It's always possible to get non-representative samples from the distribution. In this case though the ERM achieves zero error on the training set, it doesn't do a good job predicting over the true distribution. Therefore we would give a <strong><em>confidence parameter</em></strong> of tolerance:</p>
<ul>
<li><span class="math inline">\(\delta\)</span> , which denotes the possibility of getting a bad sample.</li>
</ul>
<p>However, even when we get a pretty representative sample, since it's not the whole of the distribution, it's likely that it doesn't capture all of the detail of the distribution. Again we would give an <strong><em>accuracy parameter</em></strong> of tolerance:</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span> , which denotes the error rate of the hypothesis in the true distribution.</li>
</ul>
<p><br></p>
<hr>
<p>We want to upperbound the probability of getting a <strong>misleading hypothesis</strong>. These hypotheses achieve 0 error on the training set but error &gt; <span class="math inline">\(\epsilon\)</span> on the distribution <span class="math inline">\(D\)</span>.</p>
<h3 id="proof"><strong>Proof</strong></h3>
<p>We define <span class="math inline">\(H_B\)</span> as the set of hypothesis that has an error &gt; <span class="math inline">\(\epsilon\)</span> on the distribution.</p>
<ul>
<li><span class="math inline">\(H_B\)</span> = {<span class="math inline">\(h \in H : L_{(D,f)}(h) &gt; \epsilon\)</span>}</li>
</ul>
<p>Therefore, the probability of getting a misleading hypothesis is</p>
<ul>
<li><span class="math inline">\(D^m\)</span>(<span class="math inline">\(\cup_{h\in H_B}\)</span>{<span class="math inline">\(S|_x: L_S(h) = 0\)</span>})</li>
</ul>
<p>Apply union bound to it, we get that the probability above is upperbounded by</p>
<ul>
<li><span class="math inline">\(\sum_{h\in H_B}\)</span> <span class="math inline">\(D^m\)</span>({<span class="math inline">\(S|_x: L_S(h) = 0\)</span>})</li>
</ul>
<p>The error of getting an individual bad hypothesis is</p>
<ul>
<li><span class="math inline">\((1 - L_{(D,f)}(h))^m\)</span> <span class="math inline">\(\leq (1 - \epsilon)^m\)</span> <span class="math inline">\(&lt;= e^{-\epsilon m}\)</span></li>
<li><span class="math inline">\(1 - L_{(D,f)}(h)\)</span> is the probability of get an individual misleading example</li>
</ul>
<p>Hence, we substitute the bound we got in the inequality above to the sum to obtain</p>
<ul>
<li><span class="math inline">\(|H_B|\)</span> <span class="math inline">\(e^{-\epsilon m}\)</span></li>
<li><span class="math inline">\(|H_B|\)</span> is the number of bad hypothesis which isn't known, we again upperbound it by the total number of hypothesis - <span class="math inline">\(|H|\)</span>.</li>
</ul>
<p>Putting all the things together, we have an upperbound that is</p>
<ul>
<li><span class="math inline">\(D^m\)</span>(<span class="math inline">\(\cup_{h\in H}\)</span>{<span class="math inline">\(S|_x: L_{(D,f)}(h) &gt; \epsilon,\)</span> <span class="math inline">\(L_S(h) = 0\)</span>}) <span class="math inline">\(\leq |H|\)</span> <span class="math inline">\(e^{-\epsilon m}\)</span></li>
</ul>
<hr>
<p>In the case that <span class="math inline">\(H\)</span> is finite, we can apply a lowerbound on <span class="math inline">\(m\)</span> given the condifence parameter <span class="math inline">\(\delta\)</span> and accuracy parameter <span class="math inline">\(\epsilon\)</span>.</p>
<p>If we want to have at most $$ odds on getting a misleading hypothesis</p>
<p><strong>i.e.</strong> at least $1 - $ confidence of getting a hypothesis that's approximately correct(<span class="math inline">\(L_{(D,f)}(h) &lt; \epsilon\)</span>)</p>
<ul>
<li><span class="math inline">\(1 - |H|\)</span> <span class="math inline">\(e^{-\epsilon m}\)</span> <span class="math inline">\(&gt;= 1 - \delta\)</span> , which gives the result below</li>
<li><span class="math inline">\(m \geq \frac{log({|H|}/{\delta})}{\epsilon}\)</span></li>
</ul>
<p>We define <span class="math inline">\(m_H (\epsilon, delta)\)</span> as <strong>sample complexity</strong></p>
<ul>
<li><span class="math inline">\(m_H (\epsilon, \delta) \leq \frac{log({|H|}/{\delta})}{\epsilon}\)</span></li>
</ul>
<p>which denotes the minimum number of examples for any ERM learned from an i.i.d sample to be</p>
<ul>
<li><strong>probably</strong>(with confidence <span class="math inline">\(&gt; 1 - \delta\)</span>) <strong>approximately</strong> (<span class="math inline">\(L_{D,f}(h) &lt; \epsilon\)</span>) <strong>correct</strong>.</li>
</ul>
<p>This is where <strong>PAC</strong> comes from.</p>
<hr>
<p>Assume that the realizability assumption holds for <span class="math inline">\(H, D, f\)</span></p>
<p>and <span class="math inline">\(m &gt;= m_H(\epsilon, \delta)\)</span> i.i.d. examples are sampled from <span class="math inline">\(D\)</span>.</p>
<p>Then if given <span class="math inline">\(\forall\)</span> <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> , a learning algorithm(<strong>e.g.</strong> ERM) return a hypothesis <span class="math inline">\(h\)</span> such that with confidence <span class="math inline">\(1 - \delta\)</span>, its <span class="math inline">\(L_{D,f}(h) &lt; \epsilon\)</span>, we say that it is PAC learnable.</p>
<p><em>Corollary</em>: Every finite class hypothesis is PAC learnable with sample complexity</p>
<ul>
<li><span class="math inline">\(m_H (\epsilon, \delta) &lt;= \frac{log({|H|}/{\delta})}{\epsilon}\)</span></li>
</ul>
<p>This doesn't mean that infinite classes aren't learnable. Some, such as axis-aligned rectangles and concentric circles in the exercises of the book are examples of the learnable ones.</p>
<hr>
<h2 id="agnostic-pac-learnable"><strong>Agnostic PAC learnable</strong></h2>
<p>Some people claim that they know how to see if a watermelon is tasty or not really well. Let's say they evaluate, or, make the prediction based on the following two features - weight and diameter. Though in reality it's impossible to have two identical watermelons, it's theoretically possible that some watermelons have the same numerical value of the two features, yet turn out that one is tasty while the other is not. Think about it in real life. Two watermelons can look really similar but taste very differently.</p>
<p>I guess this is not a perfect example because tastiness is subjective, but let's assume that there's a universal standard of measuring tastiness. In that case, if we know everything about the watermelon, everything about every single piece of physical matter it is composed of, then I guess there exist a hypothesis that can generate perfect prediction anytime. However, this is obviously impossible, and the features given to us are usually <strong>not deterministic</strong>.</p>
<p>Then there's no hypothesis that can give a correct definite answer. All the hypotheses can do is to give a <strong>distribution</strong> given the input features.</p>
<p>For example, given that the watermelon weigh 12 kilos and have a diameter of 30 centimeters, the best possible prediction is to say that it has a 70% chance of being tasty and 30% chance that it tastes bad. This predictor is based on the labels on the distribution, and we can't do better than that. We call this predictor <strong>The Bayes Optimal Predictor</strong>.</p>
<p>We can clearly see that the realizability assumption doesn't hold here, because</p>
<ul>
<li><span class="math inline">\(min_{h\in H} L_D(h)\)</span> &gt; 0</li>
</ul>
<p>We also want to define a new empirical error and a new true error:</p>
<p><strong>New Empirical Error:</strong></p>
<ul>
<li><span class="math inline">\(L_S(h) = \frac{| \{i | h(x_i) \neq y_i)\}|}{m}\)</span></li>
<li>Comparing to the original definition of empirical error <span class="math inline">\(L_S(h) = \frac{| \{i | h(x_i) \neq h_*(i)\}|}{m}\)</span> , we see that the <span class="math inline">\(h_*(i)\)</span> is replaced by <span class="math inline">\(y_i\)</span> because there's no <span class="math inline">\(h_*(i)\)</span> that predicts everything right anymore.</li>
</ul>
<p><strong>New True Error:</strong></p>
<ul>
<li><span class="math inline">\(L_D(h) = P_{(x,y)\sim D} [h(x) \neq y]\)</span></li>
<li>Notice that <span class="math inline">\(y\)</span> , which is the label, is also drawn from the distribution, which is probablistic.</li>
</ul>
<p><br></p>
<p>In this more general case of prediction task, we want to define <strong>Agnostic PAC Learnability</strong>:</p>
<p>The definition is exactly the same as PAC learnability, except that we introduce an extra term <span class="math inline">\(min_{h\in H}L_D(h)\)</span> into the inequality of PAC.</p>
<ul>
<li><span class="math inline">\(P(L_{D,f}(h) -min_{h\in H} L_D(h) \leq \epsilon)\)</span> <span class="math inline">\(\geq 1-\delta\)</span></li>
<li>The only difference is the <span class="math inline">\(min_{h\in H}L_D(h)\)</span> term</li>
<li>Our goal now is to get close to the accuracy of the Bayes Optimal Predictor but not 0 error rate which is unrealistic</li>
</ul>
<p><br></p>
<p>So now you know, when the people who claim to be expert in watermelon got you bad watermelons from the grocery store, it may not be because of they're lying. In fact they might be making the optimal decision but just happened to be unlucky.</p>
<hr>
<h3 id="steps-to-add-latex-support-to-your-hexo-blog"><strong>Steps to add LaTeX support to your hexo blog:</strong></h3>
<p>First download Pandoc,</p>
<blockquote>
<p>brew install Pandoc</p>
</blockquote>
<p>Then install hexo-renderer-pandoc,</p>
<blockquote>
<p>npm uninstall hexo-renderer-marked --save npm install hexo-renderer-pandoc --save</p>
</blockquote>
<p>After that, as usual, do</p>
<blockquote>
<p>hexo clean</p>
<p>hexo generate</p>
<p>hexo deploy</p>
</blockquote>
<p>and you're good to go.</p>
<p><br></p>
<p>Reference | https://zhuanlan.zhihu.com/p/35988761</p>
<hr>
<p>吐槽：</p>
<p>刚听到这个名字的时候笑出了声，因为翻译成中文大概是”我估摸着这大概差不多是对的吧“这种感觉。</p>
<p>鲁迅曾经写过，”大约孔乙己的确死了“，或有异曲同工之妙。</p>
<hr>
<p><em>Shai Shalev-Shwartz and Shai Ben-David. 2014.</em> <em>Understanding Machine Learning: From Theory to Algorithms</em>. <em>Cambridge University Press, USA.</em></p>
<p>Mathematics of Machine Learning at USC, taught by Guillermo Reyes Souto</p>
<p>Cover by <a href="https://www.pixiv.net/member.php?id=47488" target="_blank" rel="noopener">飴村</a> on Pixiv</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">Comments</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'inkbin'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2020/02/11/Probably-Approximately-Correct/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2020/02/11/Probably-Approximately-Correct/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//inkbin.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy; <a href="https://inkbean.github.io" style="border-bottom: none;">AR's Blog</a></li>
                <li>Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Theme: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_uv">有<span id="busuanzi_value_site_uv"></span>个小伙伴来过</span>
			
        </div>
    </div>
</body>



 	
</html>
